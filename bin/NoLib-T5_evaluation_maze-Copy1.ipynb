{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c04b2f4a-f799-41d5-90c8-ac3c6e88b4ca",
   "metadata": {},
   "source": [
    "# Evaluate Models \n",
    "\n",
    "- enumerative search\n",
    "- neural guided search\n",
    "- CodeT5\n",
    "- LibT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "118d2817-0d61-461c-b0d3-0d56a23ed133",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import dill\n",
    "try:\n",
    "    import binutil  # required to import from dreamcoder modules\n",
    "except ModuleNotFoundError:\n",
    "    import bin.binutil  # alt import if called as module\n",
    "\n",
    "from dreamcoder.task import Task\n",
    "from dreamcoder.dreamcoder import *\n",
    "from dreamcoder.domains.minigrid.primitives import basePrimitives, tmap, taction, idx_to_action, tdirection\n",
    "from dreamcoder.grammar import Grammar\n",
    "from dreamcoder.utilities import testTrainSplit, eprint, numberOfCPUs\n",
    "from dreamcoder.type import arrow\n",
    "from dreamcoder.domains.minigrid.nn_model_maze import *\n",
    "from dreamcoder.dreamcoder import commandlineArguments\n",
    "from dreamcoder.utilities import numberOfCPUs\n",
    "import transformers\n",
    "from transformers import RobertaTokenizer, T5ForConditionalGeneration, AutoTokenizer, TrainingArguments, Seq2SeqTrainer\n",
    "from bin.maze_T5 import parseData, all_equal, createTestDataFromTasks, get_latest_checkpoint_path, LookupTableCollator, run_on_input_examples\n",
    "Grammar.uniform(basePrimitives())\n",
    "os.environ[\"WANDB_PROJECT\"] = \"T5-Minigrid-Maze\"\n",
    "\n",
    "def makeTasks(data, chunkSize):\n",
    "    keys = data.groups.keys()\n",
    "    print('keys:', len(keys))\n",
    "    tasks = []\n",
    "    for key in keys:\n",
    "        to_imitate = data.get_group(key)\n",
    "        examples = []\n",
    "        part = 0\n",
    "        for _, row in to_imitate.iterrows():\n",
    "            input_ex = (row.obs.astype(int).tolist(), int(row['obs direction'],))\n",
    "            output_ex = int(row.action)\n",
    "            examples.append((input_ex, output_ex))\n",
    "            if chunkSize > 0 and chunkSize <= len(examples):\n",
    "                # we check that the chosen actions are not all the same\n",
    "                # otherwise it is too easy to find a program if all actions/output examples are the same\n",
    "                # this results in programs such as (lambda (lambda forward-action))\n",
    "                all_chosen_actions = list(zip(*examples))[1]\n",
    "                if not all_equal(all_chosen_actions):\n",
    "                    tasks.append(Task(f'perfect maze {key} size {chunkSize} part {part}',\n",
    "                                 arrow(tmap, tdirection, taction), examples))\n",
    "                    part += 1\n",
    "                    # we reset examples and add new chunkSize taskss\n",
    "                    examples = []\n",
    "\n",
    "    print(f'Created {len(tasks)} tasks with {chunkSize} chunk size')\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd983ed5-8f18-45c3-8cdf-df3d1b5cb276",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"/home/ma/e/eberhardinger/workspaces/ec/dreamcoder/domains/perfect-maze-minigrid/collected_data/2022-12-10T15:26:33.798573.npy\"\n",
    "\n",
    "def generate_samples_with_temp(model, tokenizer, collator, txt, n_samples, temp):\n",
    "    to_tokenizer = [txt for i in range(n_samples)]\n",
    "    outputs = model.generate(collator.encode_obs(to_tokenizer).to(\n",
    "        'cuda'), do_sample=True, max_length=128, temperature=temp)\n",
    "    results = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return results\n",
    "\n",
    "def test_programs_on_task(model, tokenizer, collator, task, grammar, n=5, temp=1.0, verbose=False):\n",
    "    progs = generate_samples_with_temp(model, tokenizer, collator, task[0], n, temp)\n",
    "    found_progs = []\n",
    "    for i, prog in enumerate(progs):\n",
    "        if verbose:\n",
    "            eprint(prog)\n",
    "        log_prior = run_on_input_examples(task[1], prog, grammar, verbose=verbose)\n",
    "        if log_prior is not None:\n",
    "            found_progs.append((Program.parse(prog), log_prior))\n",
    "\n",
    "    if len(found_progs) == 0:\n",
    "        return None, -1\n",
    "\n",
    "    found_progs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    best = found_progs[0]\n",
    "    return best[0], best[1]\n",
    "\n",
    "def check_test_tasks(model, tokenizer, collator, testTasks, grammar, n_sampling=100, verbose=False):\n",
    "    stats = []\n",
    "    solved = 0\n",
    "    processed = 0\n",
    "    for tt in (pbar := tqdm(testTasks)):\n",
    "        p, n = test_programs_on_task(model, tokenizer, collator, tt, grammar, n=n_sampling, verbose=verbose)\n",
    "        stats.append((p, tt))\n",
    "        processed += 1\n",
    "        if p is not None:\n",
    "            solved += 1\n",
    "        pbar.set_description(f\"Rate {solved}/{processed}\")\n",
    "    return stats\n",
    "\n",
    "def evaluate_enumerative_search(testingTasks, path):\n",
    "    with open(path, \"rb\") as handle:\n",
    "        result = dill.load(handle)\n",
    "    resume = len(result.grammars) - 1\n",
    "    eprint(\"Loaded checkpoint from\", path)\n",
    "    grammar = result.grammars[-1] if result.grammars else grammar\n",
    "    args = commandlineArguments(\n",
    "        enumerationTimeout=720,\n",
    "        structurePenalty=1.5,\n",
    "        recognitionSteps=5000,\n",
    "        biasOptimal=False,\n",
    "        contextual=False,\n",
    "        a=3,\n",
    "        topK=5,\n",
    "        iterations=1,\n",
    "        useRecognitionModel=True,\n",
    "        helmholtzRatio=0.5,\n",
    "        featureExtractor=MinigridMazeFeatureExtractor,\n",
    "        maximumFrontier=10,\n",
    "        CPUs=numberOfCPUs(),\n",
    "        pseudoCounts=30.0,\n",
    "        extras=None)\n",
    "    times = evaluateOnTestingTasks(result, testingTasks, grammar,\n",
    "                           CPUs=args.get('CPUs'), maximumFrontier=args.get('maximumFrontier'),\n",
    "                           solver=args.get('solver'),\n",
    "                           enumerationTimeout=args.get('enumerationTimeout'), evaluationTimeout=args.get('enumerationTimeout'))\n",
    "\n",
    "    return times\n",
    "\n",
    "def evaluate_T5(testingTasks, path, no_spaces=True, compress=False):\n",
    "    testTasks = createTestDataFromTasks(testingTasks, True, no_spaces=no_spaces, compress=compress)\n",
    "    checkpoint_dir = get_latest_checkpoint_path(path)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(checkpoint_dir).to('cuda')\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(checkpoint_dir)\n",
    "    collator = LookupTableCollator(tokenizer)\n",
    "    grammar_file = os.path.join(path, 'results.pkl')\n",
    "    with open(grammar_file, 'rb') as handle:\n",
    "        result = dill.load(handle)\n",
    "    grammar = [g['grammar'] for g in result.values()][-1]\n",
    "    stats = check_test_tasks(model, tokenizer, collator, testTasks, grammar, n_sampling=100, verbose=False)\n",
    "    solved = [x for x in stats if x[0] is not None]\n",
    "    return len(solved)\n",
    "    \n",
    "\n",
    "def evaluate_model(data_file, path, method, results_path):\n",
    "    data = np.load(data_file, allow_pickle=True)\n",
    "    parsed_data = parseData(data)\n",
    "    sequence_lengths = range(5, 31)\n",
    "    solved_tasks = []\n",
    "    idx = []\n",
    "    for i in sequence_lengths:\n",
    "        tasks = makeTasks(parsed_data, i)\n",
    "        hits = method(tasks, path)\n",
    "        solved_tasks.append({\n",
    "            'solved': hits,\n",
    "            'tasks': len(tasks)\n",
    "        })\n",
    "        idx.append(i)\n",
    "        df = pd.DataFrame(solved_tasks, index=idx)\n",
    "        df.to_csv(results_path)  \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6f7face-5a6a-4f6d-b54e-27620d793e87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1721, 6)\n",
      "keys: 18\n",
      "Created 320 tasks with 5 chunk size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate 189/320: 100%|██████████| 320/320 [1:43:08<00:00, 19.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: 18\n",
      "Created 274 tasks with 6 chunk size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate 130/274: 100%|██████████| 274/274 [1:51:22<00:00, 24.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: 18\n",
      "Created 237 tasks with 7 chunk size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate 94/237: 100%|██████████| 237/237 [1:38:48<00:00, 25.02s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: 18\n",
      "Created 206 tasks with 8 chunk size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate 65/206: 100%|██████████| 206/206 [1:53:28<00:00, 33.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: 18\n",
      "Created 181 tasks with 9 chunk size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate 59/181: 100%|██████████| 181/181 [55:56<00:00, 18.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: 18\n",
      "Created 163 tasks with 10 chunk size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate 15/58:  36%|███▌      | 58/163 [30:57<58:55, 33.67s/it]  $0 Not in candidates\n",
      "Candidates is {empty-obj: (0.0, tobj, Context(next = 1, {t0 ||> taction})), wall-obj: (0.0, tobj, Context(next = 1, {t0 ||> taction})), goal-obj: (0.0, tobj, Context(next = 1, {t0 ||> taction})), if: (0.0, bool -> tobj -> tobj -> tobj, Context(next = 2, {t1 ||> tobj, t0 ||> taction})), get: (0.0, array(array(tobj)) -> int -> int -> tobj, Context(next = 1, {t0 ||> taction}))}\n",
      "request is tobj\n",
      "xs []\n",
      "environment [tdirection, array(array(tobj))]\n",
      "Rate 40/163: 100%|██████████| 163/163 [1:18:43<00:00, 28.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: 18\n",
      "Created 150 tasks with 11 chunk size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate 24/110:  73%|███████▎  | 110/150 [43:36<22:23, 33.60s/it]PANIC: not enough arguments for the type\n",
      "request bool\n",
      "tp tobj -> tobj -> bool\n",
      "expression (eq-obj? wall-obj)\n",
      "xs [wall-obj]\n",
      "argumentTypes [tobj, tobj]\n",
      "PANIC: Grammar failure, exporting to  failures/grammarFailure1680155717.2207563.pickle\n",
      "Rate 32/150: 100%|██████████| 150/150 [1:05:03<00:00, 26.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: 18\n",
      "Created 135 tasks with 12 chunk size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate 20/135: 100%|██████████| 135/135 [43:59<00:00, 19.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: 18\n",
      "Created 125 tasks with 13 chunk size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate 20/125: 100%|██████████| 125/125 [1:02:41<00:00, 30.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: 18\n",
      "Created 114 tasks with 14 chunk size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate 17/114: 100%|██████████| 114/114 [37:12<00:00, 19.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: 18\n",
      "Created 106 tasks with 15 chunk size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate 12/106: 100%|██████████| 106/106 [57:57<00:00, 32.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: 18\n",
      "Created 100 tasks with 16 chunk size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate 0/2:   2%|▏         | 2/100 [01:03<51:06, 31.30s/it]direction-2 Not in candidates\n",
      "Candidates is {empty-obj: (0.0, tobj, Context(next = 3, {t2 ||> bool, t1 ||> taction, t0 ||> taction})), wall-obj: (0.0, tobj, Context(next = 3, {t2 ||> bool, t1 ||> taction, t0 ||> taction})), goal-obj: (0.0, tobj, Context(next = 3, {t2 ||> bool, t1 ||> taction, t0 ||> taction})), if: (0.0, bool -> tobj -> tobj -> tobj, Context(next = 4, {t3 ||> tobj, t2 ||> bool, t1 ||> taction, t0 ||> taction})), get: (0.0, array(array(tobj)) -> int -> int -> tobj, Context(next = 3, {t2 ||> bool, t1 ||> taction, t0 ||> taction}))}\n",
      "request is tobj\n",
      "xs []\n",
      "environment [tdirection, array(array(tobj))]\n",
      "Rate 10/100: 100%|██████████| 100/100 [31:44<00:00, 19.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: 18\n",
      "Created 92 tasks with 17 chunk size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate 6/92: 100%|██████████| 92/92 [41:16<00:00, 26.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: 18\n",
      "Created 87 tasks with 18 chunk size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate 6/87: 100%|██████████| 87/87 [41:17<00:00, 28.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: 18\n",
      "Created 80 tasks with 19 chunk size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate 7/80: 100%|██████████| 80/80 [32:02<00:00, 24.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: 18\n",
      "Created 78 tasks with 20 chunk size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate 2/78: 100%|██████████| 78/78 [41:27<00:00, 31.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: 18\n",
      "Created 76 tasks with 21 chunk size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate 2/76: 100%|██████████| 76/76 [25:50<00:00, 20.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: 18\n",
      "Created 70 tasks with 22 chunk size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate 1/70: 100%|██████████| 70/70 [38:51<00:00, 33.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: 18\n",
      "Created 63 tasks with 23 chunk size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate 2/63: 100%|██████████| 63/63 [19:12<00:00, 18.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: 18\n",
      "Created 63 tasks with 24 chunk size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate 1/63: 100%|██████████| 63/63 [35:02<00:00, 33.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: 18\n",
      "Created 61 tasks with 25 chunk size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate 1/61: 100%|██████████| 61/61 [20:30<00:00, 20.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: 18\n",
      "Created 60 tasks with 26 chunk size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate 1/60: 100%|██████████| 60/60 [32:16<00:00, 32.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: 18\n",
      "Created 54 tasks with 27 chunk size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate 1/54: 100%|██████████| 54/54 [26:40<00:00, 29.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: 18\n",
      "Created 52 tasks with 28 chunk size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate 0/52: 100%|██████████| 52/52 [10:45<00:00, 12.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: 18\n",
      "Created 51 tasks with 29 chunk size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate 1/51: 100%|██████████| 51/51 [27:48<00:00, 32.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys: 18\n",
      "Created 48 tasks with 30 chunk size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate 0/48: 100%|██████████| 48/48 [20:26<00:00, 25.55s/it]\n"
     ]
    }
   ],
   "source": [
    "# enum search\n",
    "path = '../../../experimentOutputs/perfect-maze/2023-03-01T21:33:58.380983/maze_aic=1.0_arity=3_ET=720_it=39_MF=10_noConsolidation=False_pc=30.0_RS=10000_RW=False_solver=ocaml_STM=True_L=1.5_TRR=default_K=5_topkNotMAP=False_rec=False.pickle'\n",
    "\n",
    "path = [#'/home/ma/e/eberhardinger/workspaces/T5-experimens/flip-data/', # T5 with lib learning\n",
    "        '/home/ma/e/eberhardinger/workspaces/T5-experimens/no-lib-learning/'] # without lib\n",
    "\n",
    "for p in path:\n",
    "    evaluate_model(data_file, p, evaluate_T5, os.path.join(p, 'eval.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36de221d-0572-42e0-a006-659e609ea63b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>solved</th>\n",
       "      <th>tasks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>272</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "      <td>216</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>169</td>\n",
       "      <td>237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>129</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>105</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>78</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>11</td>\n",
       "      <td>69</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12</td>\n",
       "      <td>45</td>\n",
       "      <td>135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>13</td>\n",
       "      <td>38</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14</td>\n",
       "      <td>27</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>15</td>\n",
       "      <td>24</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16</td>\n",
       "      <td>21</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>18</td>\n",
       "      <td>13</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0  solved  tasks\n",
       "0            5     272    320\n",
       "1            6     216    274\n",
       "2            7     169    237\n",
       "3            8     129    206\n",
       "4            9     105    181\n",
       "5           10      78    163\n",
       "6           11      69    150\n",
       "7           12      45    135\n",
       "8           13      38    125\n",
       "9           14      27    114\n",
       "10          15      24    106\n",
       "11          16      21    100\n",
       "12          17      11     92\n",
       "13          18      13     87\n",
       "14          19      11     80\n",
       "15          20       3     78\n",
       "16          21       6     76\n",
       "17          22       4     70\n",
       "18          23       1     63\n",
       "19          24       4     63\n",
       "20          25       2     61\n",
       "21          26       1     60\n",
       "22          27       1     54\n",
       "23          28       0     52\n",
       "24          29       1     51\n",
       "25          30       0     48"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('/home/ma/e/eberhardinger/workspaces/T5-experimens/no-lib-learning/eval.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80ead8e8-a29e-4a59-a819-1713a9b23003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1721, 6)\n",
      "keys: 18\n",
      "Created 320 tasks with 5 chunk size\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 163\u001b[0m\n\u001b[1;32m    160\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/ma/e/eberhardinger/workspaces/T5-experimens/noLib-newDsl/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    162\u001b[0m Grammar\u001b[38;5;241m.\u001b[39muniform(basePrimitives())\n\u001b[0;32m--> 163\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meval.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 145\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(data_file, path, results_path)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m sequence_lengths:\n\u001b[1;32m    144\u001b[0m     tasks \u001b[38;5;241m=\u001b[39m makeTasks(parsed_data, i)\n\u001b[0;32m--> 145\u001b[0m     hits \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_T5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m     solved_tasks\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolved\u001b[39m\u001b[38;5;124m'\u001b[39m: hits,\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtasks\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(tasks)\n\u001b[1;32m    149\u001b[0m     })\n\u001b[1;32m    150\u001b[0m     idx\u001b[38;5;241m.\u001b[39mappend(i)\n",
      "Cell \u001b[0;32mIn[1], line 107\u001b[0m, in \u001b[0;36mevaluate_T5\u001b[0;34m(testingTasks, path, no_spaces, compress)\u001b[0m\n\u001b[1;32m    104\u001b[0m testTasks \u001b[38;5;241m=\u001b[39m createTestDataFromTasks(\n\u001b[1;32m    105\u001b[0m     testingTasks, \u001b[38;5;28;01mTrue\u001b[39;00m, no_spaces\u001b[38;5;241m=\u001b[39mno_spaces, compress\u001b[38;5;241m=\u001b[39mcompress)\n\u001b[1;32m    106\u001b[0m checkpoint_dir \u001b[38;5;241m=\u001b[39m get_latest_checkpoint_path(path)\n\u001b[0;32m--> 107\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mT5ForConditionalGeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m RobertaTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(checkpoint_dir)\n\u001b[1;32m    110\u001b[0m collator \u001b[38;5;241m=\u001b[39m LookupTableCollator(tokenizer)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/jupyter/lib/python3.9/site-packages/transformers/modeling_utils.py:1811\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1806\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1807\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`.to` is not supported for `8-bit` models. Please use the model as it is, since the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1808\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1809\u001b[0m     )\n\u001b[1;32m   1810\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1811\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/jupyter/lib/python3.9/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 927\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/jupyter/lib/python3.9/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/jupyter/lib/python3.9/site-packages/torch/nn/modules/module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 602\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/jupyter/lib/python3.9/site-packages/torch/nn/modules/module.py:925\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/jupyter/lib/python3.9/site-packages/torch/cuda/__init__.py:217\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# This function throws if there's a driver initialization error, no GPUs\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# are found or any other error occurs\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    221\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import dill\n",
    "try:\n",
    "    import binutil  # required to import from dreamcoder modules\n",
    "except ModuleNotFoundError:\n",
    "    import bin.binutil  # alt import if called as module\n",
    "\n",
    "from dreamcoder.task import Task\n",
    "from dreamcoder.dreamcoder import *\n",
    "from dreamcoder.domains.minigrid.primitives import basePrimitives, tmap, taction, idx_to_action, tdirection\n",
    "from dreamcoder.grammar import Grammar\n",
    "from dreamcoder.utilities import testTrainSplit, eprint, numberOfCPUs\n",
    "from dreamcoder.type import arrow\n",
    "from dreamcoder.domains.minigrid.nn_model_maze import *\n",
    "from dreamcoder.dreamcoder import commandlineArguments\n",
    "from dreamcoder.utilities import numberOfCPUs\n",
    "import transformers\n",
    "from transformers import RobertaTokenizer, T5ForConditionalGeneration, AutoTokenizer, TrainingArguments, Seq2SeqTrainer\n",
    "from bin.maze_T5 import parseData, all_equal, createTestDataFromTasks, get_latest_checkpoint_path, LookupTableCollator, run_on_input_examples\n",
    "Grammar.uniform(basePrimitives())\n",
    "\n",
    "\n",
    "def makeTasks(data, chunkSize):\n",
    "    keys = data.groups.keys()\n",
    "    print('keys:', len(keys))\n",
    "    tasks = []\n",
    "    for key in keys:\n",
    "        to_imitate = data.get_group(key)\n",
    "        examples = []\n",
    "        part = 0\n",
    "        for _, row in to_imitate.iterrows():\n",
    "            input_ex = (row.obs.astype(int).tolist(),\n",
    "                        int(row['obs direction'],))\n",
    "            output_ex = int(row.action)\n",
    "            examples.append((input_ex, output_ex))\n",
    "            if chunkSize > 0 and chunkSize <= len(examples):\n",
    "                # we check that the chosen actions are not all the same\n",
    "                # otherwise it is too easy to find a program if all actions/output examples are the same\n",
    "                # this results in programs such as (lambda (lambda forward-action))\n",
    "                all_chosen_actions = list(zip(*examples))[1]\n",
    "                if not all_equal(all_chosen_actions):\n",
    "                    tasks.append(Task(f'perfect maze {key} size {chunkSize} part {part}',\n",
    "                                 arrow(tmap, tdirection, taction), examples))\n",
    "                    part += 1\n",
    "                    # we reset examples and add new chunkSize taskss\n",
    "                    examples = []\n",
    "\n",
    "    print(f'Created {len(tasks)} tasks with {chunkSize} chunk size')\n",
    "    return tasks\n",
    "\n",
    "\n",
    "def generate_samples_with_temp(model, tokenizer, collator, txt, n_samples, temp):\n",
    "    to_tokenizer = [txt for i in range(n_samples)]\n",
    "    outputs = model.generate(collator.encode_obs(to_tokenizer).to(\n",
    "        'cuda'), do_sample=True, max_length=128, temperature=temp)\n",
    "    results = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    return results\n",
    "\n",
    "\n",
    "def test_programs_on_task(model, tokenizer, collator, task, grammar, n=5, temp=1.0, verbose=False):\n",
    "    progs = generate_samples_with_temp(\n",
    "        model, tokenizer, collator, task[0], n, temp)\n",
    "    found_progs = []\n",
    "    for i, prog in enumerate(progs):\n",
    "        if verbose:\n",
    "            eprint(prog)\n",
    "        log_prior = run_on_input_examples(\n",
    "            task[1], prog, grammar, verbose=verbose)\n",
    "        if log_prior is not None:\n",
    "            found_progs.append((Program.parse(prog), log_prior))\n",
    "\n",
    "    if len(found_progs) == 0:\n",
    "        return None, -1\n",
    "\n",
    "    found_progs.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    best = found_progs[0]\n",
    "    return best[0], best[1]\n",
    "\n",
    "\n",
    "def check_test_tasks(model, tokenizer, collator, testTasks, grammar, n_sampling=100, verbose=False):\n",
    "    stats = []\n",
    "    solved = 0\n",
    "    processed = 0\n",
    "    for tt in (pbar := tqdm(testTasks)):\n",
    "        p, n = test_programs_on_task(\n",
    "            model, tokenizer, collator, tt, grammar, n=n_sampling, verbose=verbose)\n",
    "        stats.append((p, tt))\n",
    "        processed += 1\n",
    "        if p is not None:\n",
    "            solved += 1\n",
    "        pbar.set_description(f\"Rate {solved}/{processed}\")\n",
    "    return stats\n",
    "\n",
    "\n",
    "def evaluate_T5(testingTasks, path, no_spaces=True, compress=False):\n",
    "    testTasks = createTestDataFromTasks(\n",
    "        testingTasks, True, no_spaces=no_spaces, compress=compress)\n",
    "    checkpoint_dir = get_latest_checkpoint_path(path)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\n",
    "        checkpoint_dir).to('cuda')\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(checkpoint_dir)\n",
    "    collator = LookupTableCollator(tokenizer)\n",
    "    grammar_file = os.path.join(path, 'results.pkl')\n",
    "    with open(grammar_file, 'rb') as handle:\n",
    "        result = dill.load(handle)\n",
    "    grammar = [g['grammar'] for g in result.values()][-1]\n",
    "    stats = check_test_tasks(model, tokenizer, collator,\n",
    "                             testTasks, grammar, n_sampling=100, verbose=False)\n",
    "    solved = [x for x in stats if x[0] is not None]\n",
    "    return len(solved)\n",
    "\n",
    "\n",
    "def evaluate_model(data_file, path, results_path):\n",
    "    # first check if a csv exists and load the csv then and start after last seq lenght...\n",
    "    solved_tasks = []\n",
    "    idx = []\n",
    "    start_iter = 5\n",
    "    if os.path.exists(results_path):\n",
    "        df = pd.read_csv(results_path, index_col=0)\n",
    "        idx = list(df.index)\n",
    "        solved, all_tasks = df.to_dict('list').values()\n",
    "        for s, a in zip(solved, all_tasks):\n",
    "            solved_tasks.append({\n",
    "                'solved': s,\n",
    "                'tasks': a\n",
    "            })\n",
    "        print(f'loaded from {results_path}')\n",
    "        print('start from found csv file:', solved_tasks)\n",
    "        print('index:', idx)\n",
    "        start_iter = idx[-1] + 1\n",
    "\n",
    "    sequence_lengths = range(start_iter, 31)\n",
    "    data = np.load(data_file, allow_pickle=True)\n",
    "    parsed_data = parseData(data)\n",
    "    for i in sequence_lengths:\n",
    "        tasks = makeTasks(parsed_data, i)\n",
    "        hits = evaluate_T5(tasks, path)\n",
    "        solved_tasks.append({\n",
    "            'solved': hits,\n",
    "            'tasks': len(tasks)\n",
    "        })\n",
    "        idx.append(i)\n",
    "        df = pd.DataFrame(solved_tasks, index=idx)\n",
    "        df.to_csv(results_path)\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_file = \"/home/ma/e/eberhardinger/workspaces/ec/dreamcoder/domains/perfect-maze-minigrid/collected_data/2022-12-10T15:26:33.798573.npy\"\n",
    "\n",
    "    path = '/home/ma/e/eberhardinger/workspaces/T5-experimens/new-dsl/'\n",
    "    path = '/home/ma/e/eberhardinger/workspaces/T5-experimens/noLib-newDsl/'\n",
    "    \n",
    "    Grammar.uniform(basePrimitives())\n",
    "    df = evaluate_model(data_file, path, os.path.join(path, 'eval.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f30eaf-ede9-4dde-a93c-e0de1077344e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
