{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c04b2f4a-f799-41d5-90c8-ac3c6e88b4ca",
   "metadata": {},
   "source": [
    "# Evaluate Models \n",
    "\n",
    "- enumerative search\n",
    "- neural guided search\n",
    "- CodeT5\n",
    "- LibT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "118d2817-0d61-461c-b0d3-0d56a23ed133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import dill\n",
    "try:\n",
    "    import binutil  # required to import from dreamcoder modules\n",
    "except ModuleNotFoundError:\n",
    "    import bin.binutil  # alt import if called as module\n",
    "\n",
    "from dreamcoder.task import Task\n",
    "from dreamcoder.dreamcoder import *\n",
    "from dreamcoder.domains.minatar.primitives import basePrimitives, tmap, taction\n",
    "from dreamcoder.grammar import Grammar\n",
    "from dreamcoder.utilities import testTrainSplit, eprint, numberOfCPUs\n",
    "from dreamcoder.type import arrow\n",
    "from dreamcoder.domains.minatar.feature_extractor import *\n",
    "from dreamcoder.dreamcoder import commandlineArguments\n",
    "from dreamcoder.utilities import numberOfCPUs\n",
    "from dreamcoder.domains.minatar.feature_extractor import convert_to_task_input\n",
    "from dreamcoder.domains.minatar.utils_text_encoder import generate_samples_with_temp\n",
    "import transformers\n",
    "from transformers import RobertaTokenizer, T5ForConditionalGeneration\n",
    "from dreamcoder.domains.minatar.utilities import *\n",
    "os.environ[\"WANDB_PROJECT\"] = \"T5-Minatar\"\n",
    "\n",
    "def makeTasks(data, env_name, chunkSize, tolist=False):\n",
    "    tasks = []\n",
    "    examples = []\n",
    "    part = 0\n",
    "    states, actions, reward = data\n",
    "\n",
    "    state_action_pairs = list(zip(states, actions))\n",
    "\n",
    "    for i in range(len(state_action_pairs) - chunkSize):\n",
    "        examples = []\n",
    "        for state, action in state_action_pairs[i: i + chunkSize]:\n",
    "            input_ex = (convert_to_task_input(state, jax_data=True, tolist=tolist),)\n",
    "            output_ex = int(action)\n",
    "            examples.append((input_ex, output_ex))\n",
    "\n",
    "        # we check that the chosen actions are not all the same\n",
    "        # otherwise it is too easy to find a program if all actions/output examples are the same\n",
    "        # this results in programs such as (lambda (lambda forward-action))\n",
    "        all_chosen_actions = list(zip(*examples))[1]\n",
    "        if not all_equal(all_chosen_actions) and len(examples) == chunkSize:\n",
    "            tasks.append(Task(f'{env_name} size {chunkSize} part {part}',\n",
    "                              arrow(tmap, taction), examples))\n",
    "            part += 1\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd983ed5-8f18-45c3-8cdf-df3d1b5cb276",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_enumerative_search(testingTasks, path):\n",
    "    with open(path, \"rb\") as handle:\n",
    "        result = dill.load(handle)\n",
    "    resume = len(result.grammars) - 1\n",
    "    eprint(\"Loaded checkpoint from\", path)\n",
    "    grammar = result.grammars[-1] if result.grammars else grammar\n",
    "    args = commandlineArguments(\n",
    "        enumerationTimeout=720,\n",
    "        structurePenalty=1.5,\n",
    "        recognitionSteps=5000,\n",
    "        biasOptimal=False,\n",
    "        contextual=False,\n",
    "        a=3,\n",
    "        topK=5,\n",
    "        iterations=1,\n",
    "        useRecognitionModel=True,\n",
    "        helmholtzRatio=0.5,\n",
    "        featureExtractor=MinigridMazeFeatureExtractor,\n",
    "        maximumFrontier=10,\n",
    "        CPUs=numberOfCPUs(),\n",
    "        pseudoCounts=30.0,\n",
    "        extras=None)\n",
    "    times = evaluateOnTestingTasks(result, testingTasks, grammar,\n",
    "                           CPUs=args.get('CPUs'), maximumFrontier=args.get('maximumFrontier'),\n",
    "                           solver=args.get('solver'),\n",
    "                           enumerationTimeout=args.get('enumerationTimeout'), evaluationTimeout=args.get('enumerationTimeout'))\n",
    "\n",
    "    return times\n",
    "\n",
    "def check_test_tasks(testTasks, grammar, generate_sample_fn, n_sampling=100, verbose=False):\n",
    "    stats = []\n",
    "    solved_tasks = set()\n",
    "    solved = 0\n",
    "    for tt in (pbar := tqdm(testTasks)):\n",
    "        p, n = test_programs_on_task(tt, grammar, generate_sample_fn, n=n_sampling, verbose=False, use_multiprocess=True)\n",
    "        stats.append((p, n))\n",
    "        if 'random' not in str(tt[1]):\n",
    "            solved_tasks.add(tuple([str(tt[1]), str(p)]))\n",
    "\n",
    "        if p is not None:\n",
    "            solved += 1\n",
    "        pbar.set_description(f\"Solved: {solved}\")\n",
    "    return stats, solved_tasks, solved\n",
    "\n",
    "def evaluate_T5(testingTasks, path, iter_path, env_name, no_spaces=True, compress=False):\n",
    "    feature_extractor = MinatarFeatureExtractorToken(\n",
    "        env_name=env_name, max_steps=500, no_spaces=no_spaces, compress=compress)\n",
    "    #testingTasks = feature_extractor.create_test_tasks(3)\n",
    "    testTasks = createTestDataFromTasks(feature_extractor, testingTasks, True)\n",
    "    checkpoint_dir = get_latest_checkpoint_path(path)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(checkpoint_dir).to('cpu')\n",
    "    model = model.eval()\n",
    "    model = model.to('cuda')\n",
    "    tokenizer = RobertaTokenizer.from_pretrained('Salesforce/codet5-base')\n",
    "    grammar_file = os.path.join(path, iter_path, 'results.pkl')\n",
    "    with open(grammar_file, 'rb') as handle:\n",
    "        result = dill.load(handle)\n",
    "    grammar = [g['grammar'] for g in result.values()][-1]\n",
    "    stats, solved_tasks, solved = check_test_tasks(\n",
    "            testTasks, grammar, lambda x, y, z: generate_samples_with_temp(model, tokenizer, x, y, z, device='cuda'), n_sampling=500, verbose=True)\n",
    "    return solved\n",
    "    \n",
    "\n",
    "def evaluate_model(data, path, iter_path, env_name, method, results_path):\n",
    "    # first check if a csv exists and load the csv then and start after last seq lenght...\n",
    "    solved_tasks = []\n",
    "    idx = []\n",
    "    start_iter = 3\n",
    "    if os.path.exists(results_path):\n",
    "        df = pd.read_csv(results_path, index_col=0)\n",
    "        idx = list(df.index)\n",
    "        solved, all_tasks = df.to_dict('list').values()\n",
    "        for s, a in zip(solved, all_tasks):\n",
    "            solved_tasks.append({\n",
    "                'solved': s,\n",
    "                'tasks': a\n",
    "            })\n",
    "        print('start from found csv file:', solved_tasks)\n",
    "        print('index:', idx)\n",
    "        start_iter = idx[-1] + 1\n",
    "    \n",
    "    sequence_lengths = range(start_iter, 31)    \n",
    "    for i in sequence_lengths:\n",
    "        tasks = makeTasks(data, env_name, i)\n",
    "        print(f'created {len(tasks)} tasks for seq length {i}') \n",
    "        hits = method(tasks, path, iter_path, env_name)\n",
    "        #return method(tasks, path, env_name)\n",
    "        solved_tasks.append({\n",
    "            'solved': hits,\n",
    "            'tasks': len(tasks)\n",
    "        })\n",
    "        idx.append(i)\n",
    "        df = pd.DataFrame(solved_tasks, index=idx)\n",
    "        df.to_csv(results_path)  \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6f7face-5a6a-4f6d-b54e-27620d793e87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start from found csv file: [{'solved': 545, 'tasks': 855}, {'solved': 321, 'tasks': 940}, {'solved': 189, 'tasks': 974}, {'solved': 92, 'tasks': 984}, {'solved': 48, 'tasks': 989}, {'solved': 21, 'tasks': 990}]\n",
      "index: [3, 4, 5, 6, 7, 8]\n",
      "created 990 tasks for seq length 9\n",
      "loaded checkpoint from /home/ma/e/eberhardinger/workspaces/MinAtar/space_invaders_data_and_weights\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1a5f9ab821c4d038d8d35fad3a7f2de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/990 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m Grammar\u001b[38;5;241m.\u001b[39muniform(basePrimitives(env_name))\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p, iter_path \u001b[38;5;129;01min\u001b[39;00m path:\n\u001b[0;32m---> 17\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miter_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevaluate_T5\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meval.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[31], line 86\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(data, path, iter_path, env_name, method, results_path)\u001b[0m\n\u001b[1;32m     84\u001b[0m tasks \u001b[38;5;241m=\u001b[39m makeTasks(data, env_name, i)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreated \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(tasks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tasks for seq length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m) \n\u001b[0;32m---> 86\u001b[0m hits \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miter_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m#return method(tasks, path, env_name)\u001b[39;00m\n\u001b[1;32m     88\u001b[0m solved_tasks\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolved\u001b[39m\u001b[38;5;124m'\u001b[39m: hits,\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtasks\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(tasks)\n\u001b[1;32m     91\u001b[0m })\n",
      "Cell \u001b[0;32mIn[31], line 59\u001b[0m, in \u001b[0;36mevaluate_T5\u001b[0;34m(testingTasks, path, iter_path, env_name, no_spaces, compress)\u001b[0m\n\u001b[1;32m     57\u001b[0m     result \u001b[38;5;241m=\u001b[39m dill\u001b[38;5;241m.\u001b[39mload(handle)\n\u001b[1;32m     58\u001b[0m grammar \u001b[38;5;241m=\u001b[39m [g[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrammar\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mvalues()][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 59\u001b[0m stats, solved_tasks, solved \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_test_tasks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtestTasks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerate_samples_with_temp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_sampling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m solved\n",
      "Cell \u001b[0;32mIn[31], line 35\u001b[0m, in \u001b[0;36mcheck_test_tasks\u001b[0;34m(testTasks, grammar, generate_sample_fn, n_sampling, verbose)\u001b[0m\n\u001b[1;32m     33\u001b[0m solved \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tt \u001b[38;5;129;01min\u001b[39;00m (pbar \u001b[38;5;241m:=\u001b[39m tqdm(testTasks)):\n\u001b[0;32m---> 35\u001b[0m     p, n \u001b[38;5;241m=\u001b[39m \u001b[43mtest_programs_on_task\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrammar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerate_sample_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_sampling\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_multiprocess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     stats\u001b[38;5;241m.\u001b[39mappend((p, n))\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(tt[\u001b[38;5;241m1\u001b[39m]):\n",
      "File \u001b[0;32m/home/ma/e/eberhardinger/workspaces/ec/bin/../dreamcoder/domains/minatar/utilities.py:352\u001b[0m, in \u001b[0;36mtest_programs_on_task\u001b[0;34m(task, grammar, generate_sample_fn, n, temp, verbose, use_multiprocess)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtest_programs_on_task\u001b[39m(task, grammar, generate_sample_fn, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, temp\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, use_multiprocess\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m--> 352\u001b[0m     progs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mgenerate_sample_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    353\u001b[0m     num_progs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(progs)\n\u001b[1;32m    354\u001b[0m     found_progs \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[31], line 60\u001b[0m, in \u001b[0;36mevaluate_T5.<locals>.<lambda>\u001b[0;34m(x, y, z)\u001b[0m\n\u001b[1;32m     57\u001b[0m     result \u001b[38;5;241m=\u001b[39m dill\u001b[38;5;241m.\u001b[39mload(handle)\n\u001b[1;32m     58\u001b[0m grammar \u001b[38;5;241m=\u001b[39m [g[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrammar\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m result\u001b[38;5;241m.\u001b[39mvalues()][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     59\u001b[0m stats, solved_tasks, solved \u001b[38;5;241m=\u001b[39m check_test_tasks(\n\u001b[0;32m---> 60\u001b[0m         testTasks, grammar, \u001b[38;5;28;01mlambda\u001b[39;00m x, y, z: \u001b[43mgenerate_samples_with_temp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m, n_sampling\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m solved\n",
      "File \u001b[0;32m/home/ma/e/eberhardinger/workspaces/ec/bin/../dreamcoder/domains/minatar/utils_text_encoder.py:43\u001b[0m, in \u001b[0;36mgenerate_samples_with_temp\u001b[0;34m(model, tokenizer, txt, n_samples, temp, device)\u001b[0m\n\u001b[1;32m     40\u001b[0m     to_tokenizer \u001b[38;5;241m=\u001b[39m [txt \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m)]\n\u001b[1;32m     41\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(tokenizer(to_tokenizer, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m     42\u001b[0m         device), do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, temperature\u001b[38;5;241m=\u001b[39mtemp)\n\u001b[0;32m---> 43\u001b[0m     results \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     temp \u001b[38;5;241m=\u001b[39m temp \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m0.005\u001b[39m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m/opt/miniconda3/envs/jupyter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3437\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_decode\u001b[0;34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3414\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_decode\u001b[39m(\n\u001b[1;32m   3415\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3416\u001b[0m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3419\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3420\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   3421\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3422\u001b[0m \u001b[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[1;32m   3423\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3435\u001b[0m \u001b[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[1;32m   3436\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m   3438\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(\n\u001b[1;32m   3439\u001b[0m             seq,\n\u001b[1;32m   3440\u001b[0m             skip_special_tokens\u001b[38;5;241m=\u001b[39mskip_special_tokens,\n\u001b[1;32m   3441\u001b[0m             clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[1;32m   3442\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3443\u001b[0m         )\n\u001b[1;32m   3444\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[1;32m   3445\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/jupyter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3438\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3414\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbatch_decode\u001b[39m(\n\u001b[1;32m   3415\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   3416\u001b[0m     sequences: Union[List[\u001b[38;5;28mint\u001b[39m], List[List[\u001b[38;5;28mint\u001b[39m]], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp.ndarray\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3419\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3420\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   3421\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3422\u001b[0m \u001b[38;5;124;03m    Convert a list of lists of token ids into a list of strings by calling decode.\u001b[39;00m\n\u001b[1;32m   3423\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3435\u001b[0m \u001b[38;5;124;03m        `List[str]`: The list of decoded sentences.\u001b[39;00m\n\u001b[1;32m   3436\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   3437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m-> 3438\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3439\u001b[0m \u001b[43m            \u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3440\u001b[0m \u001b[43m            \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3441\u001b[0m \u001b[43m            \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3442\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3443\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3444\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m sequences\n\u001b[1;32m   3445\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/miniconda3/envs/jupyter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3476\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3473\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3474\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3476\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3480\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3481\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/jupyter/lib/python3.9/site-packages/transformers/tokenization_utils.py:931\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decode\u001b[39m(\n\u001b[1;32m    922\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    923\u001b[0m     token_ids: List[\u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    928\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    929\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode_use_source_tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_source_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 931\u001b[0m     filtered_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_ids_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;66;03m# To avoid mixing byte-level and unicode for byte-level BPT\u001b[39;00m\n\u001b[1;32m    934\u001b[0m     \u001b[38;5;66;03m# we need to build string separately for added tokens and byte-level tokens\u001b[39;00m\n\u001b[1;32m    935\u001b[0m     \u001b[38;5;66;03m# cf. https://github.com/huggingface/transformers/issues/1133\u001b[39;00m\n\u001b[1;32m    936\u001b[0m     sub_texts \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/miniconda3/envs/jupyter/lib/python3.9/site-packages/transformers/tokenization_utils.py:907\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m ids:\n\u001b[1;32m    906\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(index)\n\u001b[0;32m--> 907\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m skip_special_tokens \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_special_ids\u001b[49m:\n\u001b[1;32m    908\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madded_tokens_decoder:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/jupyter/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1298\u001b[0m, in \u001b[0;36mSpecialTokensMixin.all_special_ids\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1295\u001b[0m \u001b[38;5;124;03m`List[int]`: List the ids of the special tokens(`'<unk>'`, `'<cls>'`, etc.) mapped to class attributes.\u001b[39;00m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1297\u001b[0m all_toks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_tokens\n\u001b[0;32m-> 1298\u001b[0m all_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_tokens_to_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_toks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_ids\n",
      "File \u001b[0;32m/opt/miniconda3/envs/jupyter/lib/python3.9/site-packages/transformers/tokenization_utils.py:579\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_tokens_to_ids\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    577\u001b[0m ids \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[0;32m--> 579\u001b[0m     ids\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_token_to_id_with_added_voc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    580\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "File \u001b[0;32m/opt/miniconda3/envs/jupyter/lib/python3.9/site-packages/transformers/tokenization_utils.py:588\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._convert_token_to_id_with_added_voc\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madded_tokens_encoder:\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madded_tokens_encoder[token]\n\u001b[0;32m--> 588\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_token_to_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/jupyter/lib/python3.9/site-packages/transformers/models/roberta/tokenization_roberta.py:303\u001b[0m, in \u001b[0;36mRobertaTokenizer._convert_token_to_id\u001b[0;34m(self, token)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_token_to_id\u001b[39m(\u001b[38;5;28mself\u001b[39m, token):\n\u001b[1;32m    302\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Converts a token (str) in an id using the vocab.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder\u001b[38;5;241m.\u001b[39mget(token, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munk_token\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# enum search\n",
    "path = '../../../experimentOutputs/perfect-maze/2023-03-01T21:33:58.380983/maze_aic=1.0_arity=3_ET=720_it=39_MF=10_noConsolidation=False_pc=30.0_RS=10000_RW=False_solver=ocaml_STM=True_L=1.5_TRR=default_K=5_topkNotMAP=False_rec=False.pickle'\n",
    "\n",
    "\n",
    "env_name = 'space_invaders' \n",
    "data = np.load(\n",
    "        f\"/home/ma/e/eberhardinger/workspaces/gymnax-blines/notebooks/{env_name}/rollouts.npy\", allow_pickle=True)[0]\n",
    "\n",
    "path = [#'/home/ma/e/eberhardinger/workspaces/T5-experimens/flip-data/', # T5 with lib learning\n",
    "        #'/home/ma/e/eberhardinger/workspaces/T5-experimens/no-lib-learning/'\n",
    "        (f'/home/ma/e/eberhardinger/workspaces/T5-{env_name}-new_DSL/{env_name}/allActions-noLib-20000p-500s', 'iter-11_seqlength-7') ,\n",
    "        (f'/home/ma/e/eberhardinger/workspaces/T5-{env_name}-new_DSL/{env_name}/allActions-20000p-500s', 'iter-10_seqlength-6') # T5-{env_name}-new_DSL\n",
    "        ] # T5-{env_name}-new_DSL\n",
    "\n",
    "Grammar.uniform(basePrimitives(env_name))\n",
    "for p, iter_path in path:\n",
    "    df = evaluate_model(data, p, iter_path, env_name, evaluate_T5, os.path.join(p, 'eval.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dc4a4f-b8bd-4216-9955-7c4e66b51827",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tasks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59a765e-d9b9-4c78-9942-6be31d616da4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
